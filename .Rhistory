otu.ral <- log(otu.ra)
```{r, include = TRUE, echo = TRUE}
#> Check to see if the row names of the dat and otu object match.
all(row.names(dat) == row.names(otu.ral))
model =
'
m =~ p0 + p1 + p2
outcome ~ m
'
#> Sub-matrices creation based on correlation bins.
dat_sub <- submatrices(otu.ral, dat$duration, range = "both", by= 0.01, j = 3, method = "pearson")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Calculate summary statistics for best parcel for all submatrices.
cfa_compare <- cfa_wrap(parc, model, otu)
#> Here we only grab the negative correlations.
neg <- cfa_compare[grep("negative", names(dat_sub)),]
#> Now we order our dataframe by the parceling scheme with the best r2 and largest percent of sites of the data.
neg <- euclid_optimize("r2", "percent_of_sites", neg)
neg
#> Next we tune the parcelng scheme for the optimal and validated submatrix.
best <- tune_parcel(model, dat.o = neg, dat.l = dat_sub, dat.p = parc, y = dat$duration, n = 1000, k = 3, optimizer = "r2")
inspect(best[[1]], "r2")
standardizedsolution(best[[1]])
obs_r2 <- unname(inspect(best[[1]], "r2")[4])
cor_threshold <- as.numeric(sub(".* ", "", cfa_compare$dataset[1]))
obs_r2
inspect(best[[1]], "r2")
model =
'
m =~ p0 + p1 + p2
outcome ~ m
'
#> Sub-matrices creation based on correlation bins.
dat_sub <- submatrices(otu.ral, dat$duration, range = "both", by= 0.01, j = 3, method = "pearson")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Calculate summary statistics for best parcel for all submatrices.
cfa_compare <- cfa_wrap(parc, model, otu)
#> Here we only grab the negative correlations.
neg <- cfa_compare[grep("negative", names(dat_sub)),]
#> Now we order our dataframe by the parceling scheme with the best r2 and largest percent of sites of the data.
neg <- euclid_optimize("r2", "percent_of_sites", neg)
neg
#> Next we tune the parcelng scheme for the optimal and validated submatrix.
best <- tune_parcel(model, dat.o = neg, dat.l = dat_sub, dat.p = parc, y = dat$duration, n = 1000, k = 3, optimizer = "r2")
inspect(best[[1]], "r2")
standardizedsolution(best[[1]])
obs_r2 <- unname(inspect(best[[1]], "r2")[4])
cor_threshold <- as.numeric(sub(".* ", "", cfa_compare$dataset[1]))
obs_r2
cor_threshold
cfa_compare
standardizedsolution(best[[1]])
inspect(best[[1]], "r2")
neg
best[[1]]
best
View(best)
cfa_compare$dataset[1]
cfa_compare$dataset
inspect(best[[1]], "r2")
standardizedsolution(best[[1]])
neg
neg$dataset
neg$dataset[1]
cor_threshold <- as.numeric(sub(".* ", "", neg$dataset[1]))
cor_threshold
data("dat")
dat <- dat
data("otu")
otu <- otu
#> Here we make our log-transformed OTU object. Adding 0.000001 to account for zeros in the dataset.
otu.t <- otu + .0000001
otu.ra <- otu.t/rowSums(otu.t)
otu.ral <- log(otu.ra)
#> Check to see if the row names of the dat and otu object match.
all(row.names(dat) == row.names(otu.ral))
model =
'
m =~ p0 + p1 + p2
outcome ~ m
'
#> Sub-matrices creation based on correlation bins.
dat_sub <- submatrices(otu.ral, dat$duration, range = "both", by= 0.01, j = 3, method = "pearson")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Calculate summary statistics for best parcel for all submatrices.
cfa_compare <- cfa_wrap(parc, model, otu)
#> Here we only grab the negative correlations.
neg <- cfa_compare[grep("negative", names(dat_sub)),]
#> Now we order our dataframe by the parceling scheme with the best r2 and largest percent of sites of the data.
neg <- euclid_optimize("r2", "percent_of_sites", neg)
neg
#> Next we tune the parcelng scheme for the optimal and validated submatrix.
best <- tune_parcel(model, dat.o = neg, dat.l = dat_sub, dat.p = parc, y = dat$duration, n = 1000, k = 3, optimizer = "r2")
inspect(best[[1]], "r2")
standardizedsolution(best[[1]])
obs_r2 <- unname(inspect(best[[1]], "r2")[4])
cor_threshold <- as.numeric(sub(".* ", "", neg$dataset[1]))
obs_r2
cor_threshold
neg$dataset[1]
#> Read in the data
set.seed(1235)
data("dat")
dat <- dat
data("otu")
otu <- otu
#> Here we make our log-transformed OTU object. Adding 0.000001 to account for zeros in the dataset.
otu.t <- otu + .0000001
otu.ra <- otu.t/rowSums(otu.t)
otu.ral <- log(otu.ra)
model =
'
m =~ p0 + p1 + p2
outcome ~ m
'
#> Sub-matrices creation based on correlation bins.
dat_sub <- submatrices(otu.ral, dat$duration, range = "both", by= 0.01, j = 3, method = "pearson")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Calculate summary statistics for best parcel for all submatrices.
cfa_compare <- cfa_wrap(parc, model, otu)
#> Here we only grab the negative correlations.
neg <- cfa_compare[grep("negative", names(dat_sub)),]
#> Now we order our dataframe by the parceling scheme with the best r2 and largest percent of sites of the data.
neg <- euclid_optimize("r2", "percent_of_sites", neg)
neg
#> Next we tune the parcelng scheme for the optimal and validated submatrix.
best <- tune_parcel(model, dat.o = neg, dat.l = dat_sub, dat.p = parc, y = dat$duration, n = 1000, k = 3, optimizer = "r2")
inspect(best[[1]], "r2")
standardizedsolution(best[[1]])
obs_r2 <- unname(inspect(best[[1]], "r2")[4])
cor_threshold <- as.numeric(sub(".* ", "", neg$dataset[1]))
obs_r2
cor_threshold
#> Here we permute the data 1000 times according to the n argument we define.
dat_perm = permute_matrix(otu, dat$duration, n = 100, cor.threshold = cor_threshold)
#> This is done in order to get a list of parcel statistics, otu parcel assignments, and parcels with the outcome.
#> This object is used for the cfa wrap and can be used for a multi-objective optimization figure.
parc_perm = parcel_wrap(dat_perm_n, y =dat$duration, n = 100, k = 3, optimizer = "euclidean")
cfa_compare_perm = cfa_wrap(parc_perm, model, otu)
cfa_compare_perm$r2[is.na(cfa_compare_perm$r2)] = 0
pval <- length(which(cfa_compare_perm$r2 > obs_r2))/nrow(cfa_compare_perm)
p <- ggplot(cfa_compare_perm, aes(r2)) + geom_density(adjust = 2/1) + geom_vline(xintercept = obs_r2) +
theme_bw(base_size = 15) + xlab(bquote('Null Distribution of '*~R^2*'')) + ylab("Density")
p
pval
cfa_compare_perm$r2
obs_r2
cfa_compare_perm$r2
cor_threshold
dat_perm
obs_r2
cor_threshold
length(which(cfa_compare_perm$r2 > obs_r2))/nrow(cfa_compare_perm)
nrow(cfa_compare_perm)
cfa_wrap = function(dat.p, model, otu){
#dat.p is the output from parcel_wrap
#model is a lavaan model
#otu is a community matrix of read counts
library(lavaan)
best_matrices = lapply(dat.p[2][[1]], function(x) otu[,unname(unlist(x))])
sem_comp = lapply(dat.p[3][[1]], function(x) {
x = suppressWarnings(cfa(model, x, std.lv = TRUE, missing = "fiml"))
x = data.frame(p = standardizedsolution(x)[4,7],
r2 = inspect(x, 'r2')[4])
})
sem_comp = do.call("rbind", sem_comp)
row.names(sem_comp) = NULL
sem_comp$percent_of_species = sapply(best_matrices, function(x) ncol(x)/ncol(otu))
sem_comp$percent_of_obervations = sapply(best_matrices, function(x) (sum(otu[,colnames(otu) %in% colnames(x)])/sum(otu)))
sem_comp$percent_of_sites = sapply(best_matrices, function(x) {
x = rowSums(otu[,colnames(x)])
x[x > 0] = 1
x = sum(x)/length(x)
})
sem_comp = data.frame(sapply(sem_comp, as.numeric))
sem_comp$dataset = names(dat.p[[3]])
return(sem_comp)
}
permute_matrix = function(dat, y = NULL, n, cor.threshold = NULL){
#this function is used to create a permuted community matrix where observed microbiomes are randomly assigned to sites
#optionally, these matrices can be subset to only include otus that achieve a correlation with an outcome above a provided threshold
#dat is a site x species community matrix of read counts
#y, if provided, is a response variable to that otu abundances will be compared to and retained if >< cor.threshold
#n is the number of permuted datasets to return
#cor.threshold is a value between -1 and 1 (can not be 0) specifying to keep only otus with a correlation with y > (if cor.threshold is positive) or < (if cor.threshold is negative) cor.threshold
#returns a list of randomized community matrices
if(!is.null(y) & is.null(cor.threshold)){
stop("y is provided but cor.threshold is not")
}
if(is.null(y) & !is.null(cor.threshold)){
stop("cor.threshold is provided but y is not")
}
if(!is.null(cor.threshold)){
if(cor.threshold == 0){
stop("cor.threshold cannot be 0")
}
}
dat_perm = vector("list", n)
for(i in 1:n){
hold = row.names(dat)
otu.ral = dat[sample(row.names(dat), replace = FALSE, size = nrow(dat)),]
row.names(otu.ral) = hold
otu.ral = otu.ral + .00001
otu.ral = otu.ral/rowSums(otu.ral)
otu.ral = log(otu.ral)
if(!is.null(y)){
keep = cor(otu.ral, y, use = "pairwise.complete.obs")
if(cor.threshold >= 0) {
keep = row.names(keep)[which(keep > cor.threshold)]
}
if(cor.threshold <= 0) {
keep = row.names(keep)[which(keep < cor.threshold)]
}
if(length(keep) >= 3){
otu.ral = otu.ral[,keep]
dat_perm[[i]] = otu.ral
}
}
if(is.null(y)){
dat_perm[[i]] = otu.ral
}
print(i)
}
dat_perm = dat_perm[lengths(dat_perm) != 0]
names(dat_perm) = paste0("permutation", 1:length(dat_perm), sep = "")
return(dat_perm)
}
parcel_wrap = function(dat.l, y = NULL, k, n, optimizer = "euclidean"){
#dat.l is a list of data frames or matrices of numeric data, columns of these entries are items to be parceled - typically the output of submatrices.R
#y, if provided is a numeric vector to correlate each parcel with to assess relationships
#k is the number of parcels to form from the items
#n is the number of random parceling assignment iterations
#returns a list of length 3: [1] is a data.frame of parcel statistics, [2] a list of otu parcel assignments, [3] a list of parcels with outcome
output = vector("list", 3)
best_parcel_memberships = vector("list", length(dat.l))
best_parcels = vector("list", length(dat.l))
global_df = c()
for(j in 1:length(dat.l)){
if(ncol(dat.l[[j]]) <= 6){
w = factorial(ncol(dat.l[[j]]))
print(paste("starting parceling for submatrix ", j))
df = optimal_parcels(dat.l[[j]], y, k, n = w, optimizer = optimizer)
print(paste("finished parceling for submatrix ", j))
}
if(ncol(dat.l[[j]]) > 6){
print(paste("starting parceling for submatrix ", j))
df = optimal_parcels(dat.l[[j]], y, k, n, optimizer = optimizer)
print(paste("finished parceling for submatrix ", j))
}
df[[2]]$submatrix = names(dat.l)[j]
best_parcel_memberships[[j]] = df[[1]][[as.numeric(row.names(df[[2]])[1])]]
best_parcel_memberships[[j]] = lapply(best_parcel_memberships[[j]], function(x) names(dat.l[[j]][x]))
names(best_parcel_memberships)[j] = names(dat.l)[j]
best_parcels[[j]] = lapply(best_parcel_memberships[[j]], function(x) rowMeans(dat.l[[j]][,x, drop = FALSE]))
best_parcels[[j]] = as.data.frame(do.call(cbind, best_parcels[[j]]))
names(best_parcels)[j] = names(dat.l)[j]
names(best_parcels[[j]]) = paste0("p", names(best_parcels[[j]]), sep = "")
if(!is.null(y)) {best_parcels[[j]]$outcome = y}
global_df = rbind(global_df, df[[2]])
print(paste(j, " of ", length(dat.l), " submatrices optimized",sep = ""))
}
output[1][[1]] = global_df
names(output)[1] = "scores"
output[2][[1]] = best_parcel_memberships
names(output)[2] = "memberships"
output[3][[1]] = best_parcels
names(output)[3] = "parcels"
return(output)
}
#> Here we permute the data 1000 times according to the n argument we define.
dat_perm = permute_matrix(otu, dat$duration, n = 100, cor.threshold = cor_threshold)
#> This is done in order to get a list of parcel statistics, otu parcel assignments, and parcels with the outcome.
#> This object is used for the cfa wrap and can be used for a multi-objective optimization figure.
parc_perm = parcel_wrap(dat_perm_n, y =dat$duration, n = 100, k = 3, optimizer = "euclidean")
cfa_compare_perm = cfa_wrap(parc_perm, model, otu)
cfa_compare_perm$r2[is.na(cfa_compare_perm$r2)] = 0
pval <- length(which(cfa_compare_perm$r2 > obs_r2))/nrow(cfa_compare_perm)
p <- ggplot(cfa_compare_perm, aes(r2)) + geom_density(adjust = 2/1) + geom_vline(xintercept = obs_r2) +
theme_bw(base_size = 15) + xlab(bquote('Null Distribution of '*~R^2*'')) + ylab("Density")
p
boot_pos = bootstrapped_parameters(best_pos[[2]], model, n = 100)
boot = bootstrapped_parameters(best[[2]], model, n = 100)
summary(boot)
cfa_compare_perm
View(cfa_wrap)
parc
parc[2][[1]]
tune_parcel = function(model, dat.o, dat.l, dat.p, y, n =3, k = 1000, optimizer = "r2"){
best = dat.l[[as.numeric(row.names(dat.o)[1])]]
x = optimal_parcels(best, y, n = n, k = k, optimizer = optimizer)
x = x[[1]][[as.numeric(row.names(x[[2]])[1])]]
x = lapply(x, function(x) names(best[x]))
x = lapply(x, function(x) rowMeans(best[,x, drop = FALSE]))
x = as.data.frame(do.call(cbind, x))
names(x) = paste0("p", names(x), sep = "")
x$outcome = y
test = cfa(model, x, missing = "fiml")
output = vector("list", 3)
output[[1]] = test
output[[2]] = x
output[[3]] = dat.p[[2]][as.numeric(row.names(dat.o)[1])]
return(output)
}
submatrices = function(dat, y, by = 0.01, range = "both", j = 3, method = "pearson"){
#dat is a data.frame or matrix with columns containing continuous variables to correlate with y- dat is typically a site x species community matrix
#y is a vector of a continuous variable to be correlated with dat
#by is the increment size for including dat variables based on correlation with y
#if by is set larger than the max observed cor with outcome the entire dataset (or the subset that is pos or neg cor'd with
#outcome is returned (depending on the 'range' argument))
#if 'by' is set to 0 and 'range' is set to 'both' if returns two matrices- one for pos, one for neg cor with outcome
#range is one of "negative", "positive", or "all" specifying if the community matrix should be subset
#based only among dat's that are negatively or positively correlated with y. "all" returns both.
#j is the minimum number of species required to be in a submatrix for it to be retained in the output
#method is either pearson or mic and sets the type of correlation
if (!range %in% c("negative", "positive", "both"))
stop("'range' argument must be either 'negative', 'positive' or 'both'")
if(by < 0 | by > 1){
stop("'by' argument must be a value 0 >= =< 1")
}
if(!is.numeric(y)){
stop("y must be numeric")
}
if(!all(sapply(dat, is.numeric))){
stop("dat must be numeric")
}
options(scipen = 999)
if(method == "pearson"){cor_rank = data.frame(cor(dat, y, use = "pairwise.complete.obs"))}
if(method == "mic"){cor_rank = mine(dat, y, use = "pairwise.complete.obs")$MIC}
drop = c()
if(range == "negative" | range == "both"){
if(length(which(cor_rank < 0)) == 0){
stop("no correlations were negative: use range = positive")
}
if(by > abs(min(cor_rank[,1])) | by == 0){
neg_sub = vector("list", 1) #could just do neg_sub = list() I think
neg_sub[[1]] = dat[,names(which(cor_rank[,1] < 0))] #should make <=?
names(neg_sub)[1] = paste("negative ", 0, sep = "")
} else{
by.n = by
bins = seq(-1,0,by.n)
bins = bins[which(bins >= min(cor_rank[,1]))]
neg_sub = lapply(bins, function(z) z >= cor_rank)
neg_sub = lapply(neg_sub, function(z) dat[,z, drop = FALSE])
for(i in length(neg_sub):2){
if(all(suppressWarnings(colnames(neg_sub[[i]]) == colnames(neg_sub[[(i-1)]])))){
drop = c(drop, i)
}
if(ncol(neg_sub[[i]]) < 3){
drop = c(drop, i)
}
}
if(ncol(neg_sub[[1]]) < 3){
drop = c(drop, 1)
}
if(!is.null(drop)){
neg_sub = neg_sub[-drop]
bins = bins[-drop]
}
names(neg_sub) = paste("negative ", round(bins, nchar(by.n) - 2), sep = "")
}
}
drop = c()
if(range == "positive" | range == "both"){
if(length(which(cor_rank > 0)) == 0){
stop("no correlations were positive: use range = negative")
}
if(by > abs(max(cor_rank[,1])) | by == 0){
pos_sub = vector("list", 1)
pos_sub[[1]] = dat[,names(which(cor_rank[,1] > 0))] #should make >=?
names(pos_sub)[1] = paste("positive ", 0, sep = "")
} else {
by.p = by
bins = seq(0,1,by.p)
bins = bins[which(bins <= max(cor_rank[,1]))]
pos_sub = lapply(bins, function(z) z <= cor_rank)
pos_sub = lapply(pos_sub, function(z) dat[,z, drop = FALSE])
for(i in 1:(length(pos_sub)-1)){
if(all(suppressWarnings(colnames(pos_sub[[i]]) == colnames(pos_sub[[(i+1)]])))){
drop = c(drop, i)
}
if(ncol(pos_sub[[i]]) < 3){
drop = c(drop, i)
}
}
if(ncol(pos_sub[[length(pos_sub)]]) < 3){
drop = c(drop, length(pos_sub))
}
if(!is.null(drop)){
pos_sub = pos_sub[-drop]
bins = bins[-drop]
}
names(pos_sub) = paste("positive ", round(bins, nchar(by.p) - 2)   , sep = "")
}
}
if(range == "negative"){
keep = unlist(lapply(neg_sub, function(x) ncol(x) >= j))
neg_sub = neg_sub[keep]
return(neg_sub)}
if(range == "positive"){
keep = unlist(lapply(pos_sub, function(x) ncol(x) >= j))
pos_sub = pos_sub[keep]
return(pos_sub)}
if(range == "both"){
pos_sub = c(neg_sub, pos_sub)
keep = unlist(lapply(pos_sub, function(x) ncol(x) >= j))
pos_sub = pos_sub[keep]
return(pos_sub)
}
}
source("pom.R")
source("C:/Users/janci/Desktop/SEM/pom.R")
source("C:/Users/janci/Desktop/SEM/euclid_optimize.R")
source("C:/Users/janci/Desktop/SEM/submatrices.R")
source("C:/Users/janci/Desktop/SEM/optimal_parcels.R")
source("C:/Users/janci/Desktop/SEM/parcel_wrap.R")
source("C:/Users/janci/Desktop/SEM/cfa_wrap.R")
source("C:/Users/janci/Desktop/SEM/permute_matrix.R")
source("C:/Users/janci/Desktop/SEM/bootstrap.R")
source("C:/Users/janci/Desktop/SEM/bootstrapped_estimates.R")
source("C:/Users/janci/Desktop/SEM/extract_parcel.R")
source("C:/Users/janci/Desktop/SEM/tune_parcel.R")
#> Read in the data
set.seed(1235)
data("dat")
dat <- dat
data("otu")
otu <- otu
#> Here we make our log-transformed OTU object. Adding 0.000001 to account for zeros in the dataset.
otu.t <- otu + .0000001
otu.ra <- otu.t/rowSums(otu.t)
otu.ral <- log(otu.ra)
#> Check to see if the row names of the dat and otu object match.
all(row.names(dat) == row.names(otu.ral))
model =
'
m =~ p0 + p1 + p2
outcome ~ m
'
#> Sub-matrices creation based on correlation bins.
dat_sub <- submatrices(otu.ral, dat$duration, range = "both", by= 0.01, j = 3, method = "pearson")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Calculate summary statistics for best parcel for all submatrices.
cfa_compare <- cfa_wrap(parc, model, otu)
#> Here we only grab the negative correlations.
neg <- cfa_compare[grep("negative", names(dat_sub)),]
#> Now we order our dataframe by the parceling scheme with the best r2 and largest percent of sites of the data.
neg <- euclid_optimize("r2", "percent_of_sites", neg)
neg
#> Next we tune the parcelng scheme for the optimal and validated submatrix.
best <- tune_parcel(model, dat.o = neg, dat.l = dat_sub, dat.p = parc, y = dat$duration, n = 1000, k = 3, optimizer = "r2")
inspect(best[[1]], "r2")
standardizedsolution(best[[1]])
obs_r2 <- unname(inspect(best[[1]], "r2")[4])
obs_r2
best
best[[1]]
inspect(best[[1]], "r2")[4])
inspect(best[[1]], "r2")[4]
inspect(best[[1]], "r2")
cor_threshold <- as.numeric(sub(".* ", "", neg$dataset[1]))
cor_threshold
neg$dataset
#> Here we permute the data 1000 times according to the n argument we define.
dat_perm = permute_matrix(otu, dat$duration, n = 100, cor.threshold = cor_threshold)
#> This is done in order to get a list of parcel statistics, otu parcel assignments, and parcels with the outcome.
#> This object is used for the cfa wrap and can be used for a multi-objective optimization figure.
parc_perm = parcel_wrap(dat_perm_n, y =dat$duration, n = 100, k = 3, optimizer = "euclidean")
cfa_compare_perm = cfa_wrap(parc_perm, model, otu)
pval <- length(which(cfa_compare_perm$r2 > obs_r2))/nrow(cfa_compare_perm)
p <- ggplot(cfa_compare_perm, aes(r2)) + geom_density(adjust = 2/1) + geom_vline(xintercept = obs_r2) +
theme_bw(base_size = 15) + xlab(bquote('Null Distribution of '*~R^2*'')) + ylab("Density")
p
cfa_compare_perm$r2[is.na(cfa_compare_perm$r2)] = 0
pval <- length(which(cfa_compare_perm$r2 > obs_r2))/nrow(cfa_compare_perm)
p <- ggplot(cfa_compare_perm, aes(r2)) + geom_density(adjust = 2/1) + geom_vline(xintercept = obs_r2) +
theme_bw(base_size = 15) + xlab(bquote('Null Distribution of '*~R^2*'')) + ylab("Density")
p
obs_r2
cfa_compare_perm
detach("package:ParcelR", unload = TRUE)
#> Here we permute the data 1000 times according to the n argument we define.
dat_perm = permute_matrix(otu, dat$duration, n = 100, cor.threshold = cor_threshold)
#> This is done in order to get a list of parcel statistics, otu parcel assignments, and parcels with the outcome.
#> This object is used for the cfa wrap and can be used for a multi-objective optimization figure.
parc_perm = parcel_wrap(dat_perm_n, y =dat$duration, n = 100, k = 3, optimizer = "euclidean")
cfa_compare_perm = cfa_wrap(parc_perm, model, otu)
cfa_compare_perm$r2[is.na(cfa_compare_perm$r2)] = 0
pval <- length(which(cfa_compare_perm$r2 > obs_r2))/nrow(cfa_compare_perm)
p <- ggplot(cfa_compare_perm, aes(r2)) + geom_density(adjust = 2/1) + geom_vline(xintercept = obs_r2) +
theme_bw(base_size = 15) + xlab(bquote('Null Distribution of '*~R^2*'')) + ylab("Density")
#> Sub-matrices creation based on correlation bins.
dat_sub <- submatrices(otu.ral, dat$duration, range = "both", by= 0.01, j = 3, method = "pearson")
#> Optimal submatrix identification.
parc <- parcel_wrap(dat_sub, y = dat$duration, n = 1000, k = 3, optimizer = "euclidean")
#> Calculate summary statistics for best parcel for all submatrices.
cfa_compare <- cfa_wrap(parc, model, otu)
#> Here we only grab the negative correlations.
neg <- cfa_compare[grep("negative", names(dat_sub)),]
#> Now we order our dataframe by the parceling scheme with the best r2 and largest percent of sites of the data.
neg <- euclid_optimize("r2", "percent_of_sites", neg)
neg
#> Next we tune the parcelng scheme for the optimal and validated submatrix.
best <- tune_parcel(model, dat.o = neg, dat.l = dat_sub, dat.p = parc, y = dat$duration, n = 1000, k = 3, optimizer = "r2")
